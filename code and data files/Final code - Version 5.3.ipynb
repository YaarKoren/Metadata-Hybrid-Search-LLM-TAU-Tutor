{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ebH5nKSWcO6z"
   },
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 45189,
     "status": "ok",
     "timestamp": 1738683701396,
     "user": {
      "displayName": "guy weintraub",
      "userId": "05496095864938298042"
     },
     "user_tz": -120
    },
    "id": "csp-2q6_dMvF",
    "outputId": "e0361374-8328-4b94-e245-b7f11411e426"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/67.3 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.3/67.3 kB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
      "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
      "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m30.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m148.7/148.7 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m5.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.0/20.0 MB\u001b[0m \u001b[31m74.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m611.1/611.1 kB\u001b[0m \u001b[31m44.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m82.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m94.8/94.8 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m412.7/412.7 kB\u001b[0m \u001b[31m37.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.7/44.7 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m59.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m278.6/278.6 kB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m74.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.9/50.9 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m101.6/101.6 kB\u001b[0m \u001b[31m11.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m86.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m55.8/55.8 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.6/166.6 kB\u001b[0m \u001b[31m15.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m64.3/64.3 kB\u001b[0m \u001b[31m6.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m118.1/118.1 kB\u001b[0m \u001b[31m9.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.0/72.0 kB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.5/71.5 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.3/62.3 kB\u001b[0m \u001b[31m6.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m459.8/459.8 kB\u001b[0m \u001b[31m33.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m319.7/319.7 kB\u001b[0m \u001b[31m28.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.0/4.0 MB\u001b[0m \u001b[31m93.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m452.6/452.6 kB\u001b[0m \u001b[31m35.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pypika (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
      "Collecting langchain_pinecone\n",
      "  Downloading langchain_pinecone-0.2.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting aiohttp<3.11,>=3.10 (from langchain_pinecone)\n",
      "  Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.29 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (0.3.33)\n",
      "Collecting langchain-tests<0.4.0,>=0.3.7 (from langchain_pinecone)\n",
      "  Downloading langchain_tests-0.3.10-py3-none-any.whl.metadata (3.6 kB)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from langchain_pinecone) (1.26.4)\n",
      "Collecting pinecone<6.0.0,>=5.4.0 (from langchain_pinecone)\n",
      "  Downloading pinecone-5.4.2-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (25.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (6.1.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<3.11,>=3.10->langchain_pinecone) (1.18.3)\n",
      "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (6.0.2)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (1.33)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (0.3.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (24.2)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.5.2 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (2.10.6)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (9.0.0)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.11/dist-packages (from langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.25.0 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (0.28.1)\n",
      "Requirement already satisfied: pytest<9,>=7 in /usr/local/lib/python3.11/dist-packages (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (8.3.4)\n",
      "Collecting pytest-asyncio<1,>=0.20 (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone)\n",
      "  Downloading pytest_asyncio-0.25.3-py3-none-any.whl.metadata (3.9 kB)\n",
      "Collecting pytest-socket<1,>=0.6.0 (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone)\n",
      "  Downloading pytest_socket-0.7.0-py3-none-any.whl.metadata (6.7 kB)\n",
      "Collecting syrupy<5,>=4 (from langchain-tests<0.4.0,>=0.3.7->langchain_pinecone)\n",
      "  Downloading syrupy-4.8.1-py3-none-any.whl.metadata (36 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (2024.12.14)\n",
      "Collecting pinecone-plugin-inference<4.0.0,>=2.0.0 (from pinecone<6.0.0,>=5.4.0->langchain_pinecone)\n",
      "  Downloading pinecone_plugin_inference-3.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Collecting pinecone-plugin-interface<0.0.8,>=0.0.7 (from pinecone<6.0.0,>=5.4.0->langchain_pinecone)\n",
      "  Downloading pinecone_plugin_interface-0.0.7-py3-none-any.whl.metadata (1.2 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.5.3 in /usr/local/lib/python3.11/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (2.8.2)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (4.67.1)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone<6.0.0,>=5.4.0->langchain_pinecone) (2.3.0)\n",
      "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (3.7.1)\n",
      "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (1.0.7)\n",
      "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (3.10)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in /usr/local/lib/python3.11/dist-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3.0.0,>=2.5.2->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (2.27.2)\n",
      "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (2.0.0)\n",
      "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9,>=7->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (1.5.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.5.3->pinecone<6.0.0,>=5.4.0->langchain_pinecone) (1.17.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<3.11,>=3.10->langchain_pinecone) (0.2.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.29->langchain_pinecone) (3.4.1)\n",
      "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<1,>=0.25.0->langchain-tests<0.4.0,>=0.3.7->langchain_pinecone) (1.3.1)\n",
      "Downloading langchain_pinecone-0.2.2-py3-none-any.whl (11 kB)\n",
      "Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m17.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading langchain_tests-0.3.10-py3-none-any.whl (37 kB)\n",
      "Downloading pinecone-5.4.2-py3-none-any.whl (427 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m427.3/427.3 kB\u001b[0m \u001b[31m37.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_inference-3.1.0-py3-none-any.whl (87 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m87.5/87.5 kB\u001b[0m \u001b[31m8.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_interface-0.0.7-py3-none-any.whl (6.2 kB)\n",
      "Downloading pytest_asyncio-0.25.3-py3-none-any.whl (19 kB)\n",
      "Downloading pytest_socket-0.7.0-py3-none-any.whl (6.8 kB)\n",
      "Downloading syrupy-4.8.1-py3-none-any.whl (50 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.3/50.3 kB\u001b[0m \u001b[31m5.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pinecone-plugin-interface, syrupy, pytest-socket, pytest-asyncio, pinecone-plugin-inference, aiohttp, pinecone, langchain-tests, langchain_pinecone\n",
      "  Attempting uninstall: aiohttp\n",
      "    Found existing installation: aiohttp 3.11.11\n",
      "    Uninstalling aiohttp-3.11.11:\n",
      "      Successfully uninstalled aiohttp-3.11.11\n",
      "Successfully installed aiohttp-3.10.11 langchain-tests-0.3.10 langchain_pinecone-0.2.2 pinecone-5.4.2 pinecone-plugin-inference-3.1.0 pinecone-plugin-interface-0.0.7 pytest-asyncio-0.25.3 pytest-socket-0.7.0 syrupy-4.8.1\n",
      "Collecting pinecone-client\n",
      "  Downloading pinecone_client-5.0.1-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: certifi>=2019.11.17 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2024.12.14)\n",
      "Collecting pinecone-plugin-inference<2.0.0,>=1.0.3 (from pinecone-client)\n",
      "  Downloading pinecone_plugin_inference-1.1.0-py3-none-any.whl.metadata (2.2 kB)\n",
      "Requirement already satisfied: pinecone-plugin-interface<0.0.8,>=0.0.7 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (0.0.7)\n",
      "Requirement already satisfied: tqdm>=4.64.1 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (4.12.2)\n",
      "Requirement already satisfied: urllib3>=1.26.0 in /usr/local/lib/python3.11/dist-packages (from pinecone-client) (2.3.0)\n",
      "Downloading pinecone_client-5.0.1-py3-none-any.whl (244 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m244.8/244.8 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hDownloading pinecone_plugin_inference-1.1.0-py3-none-any.whl (85 kB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m85.4/85.4 kB\u001b[0m \u001b[31m9.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: pinecone-plugin-inference, pinecone-client\n",
      "  Attempting uninstall: pinecone-plugin-inference\n",
      "    Found existing installation: pinecone-plugin-inference 3.1.0\n",
      "    Uninstalling pinecone-plugin-inference-3.1.0:\n",
      "      Successfully uninstalled pinecone-plugin-inference-3.1.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "pinecone 5.4.2 requires pinecone-plugin-inference<4.0.0,>=2.0.0, but you have pinecone-plugin-inference 1.1.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed pinecone-client-5.0.1 pinecone-plugin-inference-1.1.0\n"
     ]
    }
   ],
   "source": [
    "!pip install -q -U langchain-text-splitters langchain-community langgraph langchain-openai langchain-chroma pymupdf\n",
    "!pip install langchain_pinecone\n",
    "!pip install pinecone-client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9f34gRGFdRIf"
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"PINECONE_API_KEY\"] = \"\"  # insert key here\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"\"  # insert key here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kXwriXN7Zawb"
   },
   "source": [
    "## Load Pinecone Index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "cYH-UkaYDt5u"
   },
   "outputs": [],
   "source": [
    "from pinecone import Pinecone\n",
    "\n",
    "pc = Pinecone(api_key=os.environ[\"PINECONE_API_KEY\"])\n",
    "index_pc = pc.Index(\"guy-index-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HLVWUQKUYH9z"
   },
   "outputs": [],
   "source": [
    "# Verify the Index is empty of vectors, if not, empty it\n",
    "stats = index_pc.describe_index_stats()\n",
    "if stats[\"total_vector_count\"] > 0:\n",
    "  index_pc.delete(delete_all=True) # empty index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hDPkiweeePb0"
   },
   "source": [
    "## Load LLM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XRo6vzgSePRR"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iAbnjquKeCbg"
   },
   "source": [
    "## Load Embedding Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Hf2uSeRqeALt"
   },
   "outputs": [],
   "source": [
    "from langchain_openai import OpenAIEmbeddings\n",
    "\n",
    "embeddings = OpenAIEmbeddings(model=\"text-embedding-3-large\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6D3UiOBgUWPo"
   },
   "source": [
    "# Helper Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gXMSBhndso-j"
   },
   "source": [
    "## MetaData Handling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EMB50tA2H6sh"
   },
   "source": [
    "### Date handling functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kULJsHhxH2KB"
   },
   "outputs": [],
   "source": [
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "def string_date_to_epoch_time(date_string: str) -> int:\n",
    "  '''\n",
    "  Translate string date to epoch time\n",
    "  date_string format: YYYY-MM-DD\n",
    "\n",
    "  return\n",
    "  ------\n",
    "  epoch_time\n",
    "  '''\n",
    "  date_format = \"%Y-%m-%d\"\n",
    "  dt_object = datetime.strptime(date_string, date_format)\n",
    "  epoch_time = int(dt_object.timestamp())\n",
    "  return epoch_time\n",
    "\n",
    "def epoch_time_to_date(epoch_time: int) -> str:\n",
    "  '''\n",
    "  Translate epoch time to string date\n",
    "  date_string format: YYYY-MM-DD\n",
    "\n",
    "  return\n",
    "  ------\n",
    "  date_string\n",
    "  '''\n",
    "  dt_object = datetime.utcfromtimestamp(epoch_time)\n",
    "  date_string = dt_object.strftime(\"%Y-%m-%d\")\n",
    "  return date_string\n",
    "\n",
    "def get_last_week_range():\n",
    "  '''\n",
    "  Get the start and end dates of the last week.\n",
    "  (using current time)\n",
    "\n",
    "  return\n",
    "  ------\n",
    "  start_of_last_week: date\n",
    "  end_of_last_week: date\n",
    "  '''\n",
    "  today = datetime.now()\n",
    "  start_of_this_week = today - timedelta(days=(today.weekday() + 1) % 7)\n",
    "  start_of_last_week = start_of_this_week - timedelta(weeks=1)\n",
    "  end_of_last_week = start_of_this_week - timedelta(days=1)\n",
    "  return start_of_last_week.date(), end_of_last_week.date()\n",
    "\n",
    "def get_most_recent_date(month, day):\n",
    "  '''\n",
    "  For a given month-day pair, gets the last time this date occured.\n",
    "\n",
    "  Input\n",
    "  ------\n",
    "  month: int\n",
    "  day: int\n",
    "\n",
    "  return\n",
    "  ------\n",
    "  most_recent_date: date\n",
    "  - \"current year\"-month-day\n",
    "  '''\n",
    "  today = date.today()\n",
    "  current_year_date = date(today.year, month, day)\n",
    "\n",
    "  # If the current year's date is in the future, use the previous year\n",
    "  if current_year_date > today:\n",
    "      return current_year_date.replace(year=today.year-1)\n",
    "  return current_year_date"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AfVB0WcvbGXP"
   },
   "source": [
    "### MetaData extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "d5nmx0-yUMm6"
   },
   "outputs": [],
   "source": [
    "def extract_metadata_from_df(file_name, df_metadata):\n",
    "  '''\n",
    "  Extracts metadata from the df into a dict by file_name.\n",
    "\n",
    "  return\n",
    "  -------\n",
    "  - Dict: {\n",
    "      'source_type': lecture / recitation (str)\n",
    "      'number': number (int)\n",
    "      'date': date in epoch time (int)\n",
    "      'is_summary': True / False (bool)\n",
    "    }\n",
    "  '''\n",
    "  meta_row = df_metadata[df_metadata['file_name'] == file_name]\n",
    "  source_type = meta_row.iloc[0]['source_type']\n",
    "  number = int(meta_row.iloc[0]['number'])\n",
    "  date = string_date_to_epoch_time(meta_row.iloc[0]['date']) # turn to eppoch time\n",
    "  is_summary = bool(meta_row.iloc[0]['is_summary'])\n",
    "  return {\"source_type\": source_type, \"number\": number, \"date\": date, \"is_summary\": is_summary}\n",
    "\n",
    "\n",
    "def unpack_metadata_from_result(result):\n",
    "  '''\n",
    "  Inplace change the metadata types to \"datetime.date()\" and \"int\"\n",
    "  '''\n",
    "  result.metadata[\"date\"] = epoch_time_to_date(result.metadata[\"date\"])\n",
    "  result.metadata[\"number\"] = int(result.metadata[\"number\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "tgCuBnbVcSMp"
   },
   "source": [
    "## Query Analyzer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2B-J-XO4cZpf"
   },
   "outputs": [],
   "source": [
    "def get_list_of_filter_criteria(filter: dict):\n",
    "    '''\n",
    "    For a given Filter returned from query_constructor\n",
    "    Returns a list of all the filter criteria.\n",
    "\n",
    "    examples of filters and lists:\n",
    "    {}\n",
    "      --> []\n",
    "    {'filter': {'number': {'$lt': 5}}}\n",
    "      --> [{'number': {'$lt': 5}}]\n",
    "    {'filter': {'$and': [{'is_summary': {'$eq': True}}, {'number': {'$in': [5, 8]}}]}}\n",
    "      --> [{'is_summary': {'$eq': True}}, {'number': {'$in': [5, 8]}}]\n",
    "    '''\n",
    "    if filter == {} or not isinstance(filter, dict):\n",
    "      return []\n",
    "    criteria_dict = filter['filter']\n",
    "    if '$and' in criteria_dict:\n",
    "      return criteria_dict['$and']\n",
    "    elif '$or' in criteria_dict:\n",
    "      return criteria_dict['$or']\n",
    "    else: # only one criteria\n",
    "      return [criteria_dict]\n",
    "\n",
    "\n",
    "def get_field_criteria(filter, field: str = 'date'):\n",
    "    \"\"\"\n",
    "    Check if field exists in the filter.\n",
    "    If it exists:\n",
    "      returns field_criteria_lst. a list of dics (because sometimes the same field has two)\n",
    "    Else:\n",
    "      return []\n",
    "\n",
    "    examples of dict:\n",
    "    - {}, 'date'\n",
    "     --> []\n",
    "    - {'filter': {'date': {'$gt': [{'date': '2023-09-20', 'type': 'date'}]}}}, 'date'\n",
    "     --> [{'date': {'$gt': {'date': '2023-09-20', 'type': 'date'}}}]\n",
    "    - {'filter': {'$and': [{'source_type': {'$in': ['recitation']}}, {'date': {'$lt': {'date': '2023-09-20', 'type': 'date'}}}]}}, 'source_type'\n",
    "     --> [{'source_type': {'$in': ['recitation']}]\n",
    "\n",
    "    Args\n",
    "    -----\n",
    "    - filter (dict)\n",
    "    - field: str = 'date' (can be 'number' / 'source_type' / 'is_summary')\n",
    "\n",
    "    \"\"\"\n",
    "    criteria_list = get_list_of_filter_criteria(filter)\n",
    "    field_criteria_lst = [criteria for criteria in criteria_list if field in criteria]\n",
    "    return field_criteria_lst\n",
    "\n",
    "\n",
    "def update_date_in_filter_to_epoch(filter: dict):\n",
    "    \"\"\"\n",
    "    Check if 'date' exists in the filter (within logical operators or standalone)\n",
    "    Updates structure and converts to epoch time (int).\n",
    "\n",
    "    examples of dict:\n",
    "    - {}\n",
    "     --> {}\n",
    "    - {'filter': {'date': {'$gt': [{'date': '2023-09-20', 'type': 'date'}]}}}\n",
    "     --> {'filter': {'date': {'$gt': 1695168000}}}\n",
    "    - {'filter': {'$and': [{'source_type': {'$in': ['recitation']}}, {'date': {'$lt': [{'date': '2023-09-20', 'type': 'date'}]}}]}}\n",
    "     --> {'filter': {'$and': [{'source_type': {'$in': ['recitation']}}, {'date': {'$lt': 1695168000}}]}}\n",
    "    - {'filter': {'$and': [{'source_type': {'$in': ['recitation']}}, {'date': {'$gte': {'date': '2024-12-29', 'type': 'date'}}}, {'date': {'$lte': {'date': '2025-01-04', 'type': 'date'}}}]}}\n",
    "     --> ...\n",
    "\n",
    "    Args\n",
    "    -----\n",
    "    - translation (tuple): A tuple containing a query (string) and a filter (dict).\n",
    "    \"\"\"\n",
    "    # get 'date' field from filter\n",
    "    date_criteria_list = get_field_criteria(filter, field='date')\n",
    "    if len(date_criteria_list)>0: # if date field exists\n",
    "      for date_criteria in date_criteria_list:\n",
    "        field_inner_dict = date_criteria['date'] # {'$lt': [{'date': '2023-09-20', 'type': 'date'}]}\n",
    "        for comparator, value in field_inner_dict.items():\n",
    "          if not isinstance(value, int):\n",
    "            if not isinstance(value, str):\n",
    "              str_date = value['date']\n",
    "            else:\n",
    "              str_date = value\n",
    "            epoch_date = string_date_to_epoch_time(str_date)\n",
    "            field_inner_dict[comparator] = epoch_date\n",
    "\n",
    "\n",
    "def update_field_criteria_to_filter(filter: dict, field: str='is_summary', comparator: str = '$eq', val=True):\n",
    "    \"\"\"\n",
    "    Check if field exists in the filter.\n",
    "    If exists:\n",
    "      update: field: {comparator: val}\n",
    "    Else:\n",
    "      add: field: {comparator: val\n",
    "\n",
    "    examples of dict (added to 'is_summary' = True)\n",
    "    - {}\n",
    "     --> {'filter': {'is_summary': {'$eq': True}}}\n",
    "\n",
    "    - {'filter': {'date': {'$gt': 1695168000}}}\n",
    "     --> {'filter': {'$and': [{'date': {'$gt': 1695168000}}, {'is_summary': {'$eq': True}}]}}\n",
    "\n",
    "    - {'filter': {'$and': [{'date': {'$gt': 1695168000}}, {'is_summary': {'$eq': False}}]}}\n",
    "     --> {'filter': {'$and': [{'date': {'$gt': 1695168000}}, {'is_summary': {'$eq': True}}]}}\n",
    "    \"\"\"\n",
    "    field_criteria_list = get_field_criteria(filter, field)\n",
    "    if len(field_criteria_list)>=2:\n",
    "      print(\"Error - cannot update fields with 2 criteria\")\n",
    "    elif len(field_criteria_list)==1: # if field allready exists in filter\n",
    "      field_criteria_list[0][field] = {comparator: val} # update as desired\n",
    "    else: # if field does not exist in filter\n",
    "      criteria_list = get_list_of_filter_criteria(filter)\n",
    "      if len(criteria_list) == 0: # filter is currently empty\n",
    "        filter['filter'] = {field: {comparator: val}}\n",
    "      elif len(criteria_list) == 1:\n",
    "        filter['filter'] = {'$and': [{field: {comparator: val}}, criteria_list[0]]}\n",
    "      else:\n",
    "        criteria_list.append({field: {comparator: val}})\n",
    "\n",
    "\n",
    "def get_lecture_numbers_as_list(filter: dict):\n",
    "  '''\n",
    "  The assumption:\n",
    "  a number has no more one field_criteria (e.g:\n",
    "    - {'number': {'$lt': 5}}\n",
    "        return [1,2,3,4]\n",
    "    - {'number': {'$in': [5]}}\n",
    "        return [5]\n",
    "    - {'number': {'$in': [5,6,7,8]}}\n",
    "        return [5]\n",
    "\n",
    "  If has 0:\n",
    "    - return []\n",
    "\n",
    "  Assuming the first is 1 and the last is 12\n",
    "  '''\n",
    "  number_criteria_list = get_field_criteria(filter, 'number')\n",
    "  min_number = 1\n",
    "  max_number = 12\n",
    "  if len(number_criteria_list)==0: # if this field does not exist\n",
    "    return []\n",
    "  elif len(number_criteria_list) > 1: # there are 2 number criteria\n",
    "    print(\"Error - Number field should have 1 criteria only!\")\n",
    "    return\n",
    "  else: # if there is only one number criteria\n",
    "    number_criteria = number_criteria_list[0] # there is only a single number criteria\n",
    "    comparator, value = next(iter(number_criteria['number'].items()))\n",
    "    num_list = []\n",
    "    if comparator == '$in': # {'$in': [5,6,7,8]}\n",
    "      return value\n",
    "    elif comparator == '$lt': # {'$lt': 5}\n",
    "      num_list = [num for num in range(min_number,value)]\n",
    "    elif comparator == '$lte': # {'$lte': 5}\n",
    "      num_list = [num for num in range(min_number,value+1)]\n",
    "    elif comparator == '$gt':\n",
    "      num_list = [num for num in range(value+1, max_number+1)]\n",
    "    elif comparator == '$gte':\n",
    "      num_list = [num for num in range(value, max_number+1)]\n",
    "\n",
    "    return num_list\n",
    "\n",
    "\n",
    "def choose_k_by_translated_query(translated_query: tuple) -> int:\n",
    "  '''\n",
    "  Chooses the desired k by the transalted_query:\n",
    "  - translated_query[0] is the query\n",
    "  - translated_query[1] is the filter\n",
    "\n",
    "  '''\n",
    "  filter = translated_query[1]\n",
    "  # get how many numbers in filter\n",
    "  number_lst = get_lecture_numbers_as_list(filter)\n",
    "  count_numbers = len(number_lst)\n",
    "  is_summary_list = get_field_criteria(filter, field='is_summary')\n",
    "  is_summary_flag = is_summary_list[0]['is_summary']['$eq'] if (len(is_summary_list)==1) else False\n",
    "\n",
    "  if is_summary_flag:\n",
    "    if count_numbers == 0:\n",
    "      k = 5\n",
    "    elif count_numbers == 1: # one lecture to summarize\n",
    "      k = 7\n",
    "    else: # count_numbers > 1\n",
    "      k = min(15, count_numbers*5)\n",
    "  else: # if not \"general_information\"\n",
    "    if count_numbers == 0:\n",
    "      k = 5\n",
    "    elif count_numbers == 1:\n",
    "      k = 5\n",
    "    else: # count_numbers > 1\n",
    "      k = min(15, count_numbers*5)\n",
    "\n",
    "  return k\n",
    "\n",
    "\n",
    "def get_adapted_query_by_translated_query(translated_query: tuple, adapted_query: str) -> str:\n",
    "  '''\n",
    "  Uses the filter.\n",
    "  Adds a prefix to the query, with the relevant sources of documents.\n",
    "\n",
    "  '''\n",
    "  if translated_query[0] == \" \":\n",
    "    classes_list = get_lecture_numbers_as_list(translated_query[1])\n",
    "    is_query_include_date = len(get_field_criteria(translated_query[1], field='date')) > 0\n",
    "    is_query_include_number = len(classes_list) > 0\n",
    "    if is_query_include_date and is_query_include_number:\n",
    "      adapted_query = f\"The materials are from the requested dates and from classes {classes_list}. \" + adapted_query\n",
    "    elif is_query_include_date:\n",
    "      adapted_query = \"The materials are from the requested dates. \" + adapted_query\n",
    "    elif is_query_include_number:\n",
    "      adapted_query = f\"The materials are from classes {classes_list}. \" + adapted_query\n",
    "\n",
    "  return adapted_query\n",
    "\n",
    "def update_filter_by_translated_query(translated_query: tuple):\n",
    "  '''\n",
    "  If the content query is empty, add \"is_summary\"\n",
    "  * unless its a recitation\n",
    "\n",
    "  Changes filter inplace\n",
    "\n",
    "  '''\n",
    "  if translated_query[0] == \" \":\n",
    "    # currently we don't have summeries for recitations, so we won't add is_summary to those questions\n",
    "    is_summary_val = \"recitation\" not in str(get_field_criteria(translated_query[1], field='source_type'))\n",
    "    update_field_criteria_to_filter(translated_query[1], field = 'is_summary', val = is_summary_val)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oQfXUT0wfufL"
   },
   "source": [
    "## Translation and Retirval"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "icvj9y3Aq7Nz"
   },
   "source": [
    "## Printing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kgjkj2Vsq2iw"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "def print_documents_metadata_with_str_date(docs: list):\n",
    "  '''\n",
    "  Iterate over the docs list and print the metadata of each Document.\n",
    "  Print the date as date string.\n",
    "  '''\n",
    "  for i, doc in enumerate(docs):\n",
    "      metadata_copy = copy.deepcopy(doc.metadata) # deep copy\n",
    "      metadata_copy['date'] = epoch_time_to_date(metadata_copy['date'])\n",
    "      print(f\"Document {i+1} Metadata: {metadata_copy}\")\n",
    "\n",
    "def print_documents_metadata(docs: list):\n",
    "  '''\n",
    "  Iterate over the docs list and print the metadata of each Document\n",
    "  Print the date as epoch_time.\n",
    "\n",
    "  The Document.metadata is a dict:\n",
    "  {'source':file_path (str),\n",
    "    'source_type': source_type (str),\n",
    "    'number': number (int),\n",
    "    'date': date (str),\n",
    "    'is_summary': is_summary (bool)\n",
    "   }\n",
    "  '''\n",
    "  for i, doc in enumerate(docs):\n",
    "      print(f\"Document {i+1} Metadata: {doc.metadata}\")\n",
    "\n",
    "\n",
    "def print_k_chunks(all_splits: list, k: int = 5):\n",
    "  '''\n",
    "  Iterate over the first k chunks in all_splits list and print the metadata of each Document\n",
    "  '''\n",
    "  for i, split in enumerate(all_splits[:k]):\n",
    "      print(f\"Chunk {i+1}:\")\n",
    "      print(f\"Document Metadata: {split.metadata}\")\n",
    "      print(f\"Page Content: {split.page_content[:100]} [...]\\n\")  # Preview the content\n",
    "\n",
    "\n",
    "def print_query_results(results, print_embedding_flag=True):\n",
    "  '''\n",
    "  Iterate over the results returned by index.query()\n",
    "  print the metadata of each \"match\".\n",
    "  results[\"matches\"] is a list of dicts,\n",
    "  '''\n",
    "  for match in results[\"matches\"]:\n",
    "      print(f\"Vector ID: {match['id']}\")\n",
    "      print(f\"score: {match['score']}\")\n",
    "      if print_embedding_flag:\n",
    "        print(f\"Embedding: {match['values']}\")\n",
    "      print(f\"Metadata: {match['metadata']}\")\n",
    "\n",
    "\n",
    "def print_retrieved_documents(results):\n",
    "  '''\n",
    "  Iterate over the results from \"similarity_search\" (Document list)\n",
    "  print the metadata of each Document.\n",
    "\n",
    "  * if the similarity search was \"with_score\", then results is a list of tuples(Documents, score)\n",
    "  '''\n",
    "  # Check if results contain tuples (Document, score) or just Documents\n",
    "  scores_flag = isinstance(results[0], tuple) if results else False\n",
    "\n",
    "  if scores_flag:\n",
    "    for result, score in results:\n",
    "      print(\"\\n\")\n",
    "      print(f\"Similarity Score:\\t{score}\")  # Inspect similarity score\n",
    "      print(f\"Doc id:\\t{result.id}\")\n",
    "      print(f\"Doc Metadata: {result.metadata}\")\n",
    "      print(f\"Doc Content:\\n {result.page_content[:100]}\\n\")\n",
    "  else:\n",
    "    for result in results:\n",
    "      print(\"\\n\")\n",
    "      print(f\"Doc id:\\t{result.id}\")\n",
    "      print(f\"Doc Metadata: {result.metadata}\")\n",
    "      print(f\"Doc Content:\\n {result.page_content[:100]}\\n\")\n",
    "\n",
    "\n",
    "def print_parsed_query(user_query: str, query_constructor):\n",
    "  '''\n",
    "  Prints (+ returns) the parsed query (after Constructor)\n",
    "  '''\n",
    "  parsed_query = query_constructor.invoke(user_query)\n",
    "  print(parsed_query)\n",
    "  return parsed_query\n",
    "\n",
    "\n",
    "def print_translated_query(translated_query):\n",
    "  '''\n",
    "  Prints the translated query (after Constructor + Translator)\n",
    "  '''\n",
    "  print(f\"Content Query: {translated_query[0]}\")\n",
    "  print(f\"Filter: {translated_query[1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qHZTQVgrb7tf"
   },
   "source": [
    "## String Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7_W6a2z4W1yJ"
   },
   "outputs": [],
   "source": [
    "def get_filter_string(filter: dict, print_flag=False):\n",
    "  '''\n",
    "  filter examples:\n",
    "  {},\n",
    "  {'filter': {'number': {'$lt': 5}}},\n",
    "  {'filter': {'number': {'$in': [5]}}},\n",
    "  {'filter': {'date': {'$eq': 1695168000}}},\n",
    "  {'filter': {'source_type': {'$in': ['lecture']}}},\n",
    "  {'filter': {'$and': [{'source_type': {'$in': ['recitation']}}, {'number': {'$in': [5, 6]}}]}},\n",
    "  {'filter': {'$and': [{'source_type': {'$in': ['recitation']}}, {'date': {'$lt': 1695168000}}]}},\n",
    "  {'filter': {'$and': [{'is_summary': {'$eq': True}}, {'number': {'$in': [5, 8]}}]}}\n",
    "  '''\n",
    "  filter_string = \"\"\n",
    "  criteria_list = get_list_of_filter_criteria(filter)\n",
    "  if len(criteria_list) == 0:\n",
    "    filter_string = \"No filter\"\n",
    "  else:\n",
    "    for criteria in criteria_list:\n",
    "      filter_string += f\"{get_criteria_string(criteria)}\\n\"\n",
    "\n",
    "  if print_flag:\n",
    "    print(filter_string)\n",
    "  return filter_string\n",
    "\n",
    "\n",
    "def get_criteria_string(criteria: dict, print_flag=False):\n",
    "  '''\n",
    "  criteria examples:\n",
    "  {'number': {'$lt': 5}},\n",
    "  {'number': {'$in': [6, 8]}},\n",
    "  {'date': {'$eq': 1695168000}},\n",
    "  {'source_type': {'$in': ['recitation']}},\n",
    "  '''\n",
    "  for field, value_definition in criteria.items():\n",
    "    for comparator, value in value_definition.items():\n",
    "      if comparator in ['$in', '$eq']:\n",
    "        compare_str = '='\n",
    "      elif comparator == '$lt':\n",
    "        compare_str = 'before (not including)'\n",
    "      elif comparator == '$gt':\n",
    "        compare_str = 'after (not including)'\n",
    "      elif comparator == '$lte':\n",
    "        compare_str = 'before (including)'\n",
    "      elif comparator == '$gte':\n",
    "        compare_str = 'after (including)'\n",
    "      else:\n",
    "        compare_str = '[un recognised comparator]'\n",
    "      if isinstance(value, list):\n",
    "        value_string = ' / '.join(map(str, value))\n",
    "      else:\n",
    "        value_string = str(value)\n",
    "\n",
    "      if field == 'date':\n",
    "        value_string = epoch_time_to_date(value)\n",
    "\n",
    "  criteria_string = f\" * {field}\\t{compare_str} {value_string}\"\n",
    "  if print_flag:\n",
    "    print(criteria_string)\n",
    "  return criteria_string\n",
    "\n",
    "\n",
    "def get_context_string_from_retrieved_docs(docs_list: list):\n",
    "  '''\n",
    "  Formats the retrieved docs of the context as a string:\n",
    "  excerpts of:\n",
    "  lecture 6, 2024-2-5: {lkajhfkjhkjshdf}\n",
    "  summary of lecture 6, 2024-2-5: {lkajhfkjhkjshdf}\n",
    "  recitation 3, 2024-1-4: {lkajhfkjhkjshdf}\n",
    "  '''\n",
    "  context_string = 'Excerpts of:\\n\\n'\n",
    "  for doc in docs_list:\n",
    "    source_type = doc.metadata['source_type']\n",
    "    number = int(doc.metadata['number'])\n",
    "    date = epoch_time_to_date(doc.metadata['date'])\n",
    "    if doc.metadata['is_summary']:\n",
    "      context_string += f\"summary of \"\n",
    "    context_string += f\"{source_type} {number}, {date}:\\n{doc.page_content}\\n\\n\"\n",
    "  return context_string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SvPEArRiy3NI"
   },
   "source": [
    "## Data structure description\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rkHXJ3kOvMnd"
   },
   "source": [
    "**vector_store.similarity_search_with_score**\n",
    "\n",
    "returns a *list* of *tuples(Documents, score)*\n",
    "```\n",
    "[\n",
    "  (Document(\n",
    "      id='08a76c37-242f-440d-a18c-ab2dfc2d3cf3',\n",
    "      metadata={\n",
    "          'date': datetime.date(2024, 2, 5),\n",
    "          'number': 6,\n",
    "          'source': '/content/materials/texts/lecture_6a_en.txt',\n",
    "          'source_type': 'recitation'},\n",
    "      page_content=\"others.\\nThere are a lot of thing......\"),\n",
    "   0.385573089),\n",
    "  (Document(...),\n",
    "   0.385573089)\n",
    "  ...\n",
    "]\n",
    "\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6uCgEWbKeIiz"
   },
   "source": [
    "# Load Files to Vector Store (Pinecone)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EQCN2CqBi_nl"
   },
   "outputs": [],
   "source": [
    "from langchain import hub\n",
    "from langchain_community.document_loaders import TextLoader, PyMuPDFLoader\n",
    "from langchain_core.documents import Document\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langgraph.graph import START, StateGraph\n",
    "from typing_extensions import List, TypedDict\n",
    "from langchain_pinecone import PineconeVectorStore"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8K9E6NbNkkTS"
   },
   "source": [
    "## Load Files and MetaData"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UleQoUWhL6U9"
   },
   "source": [
    "**upload files**\n",
    "\n",
    "1. *hardware_materials_with_summaries.zip* - With all materials wanted\n",
    "2. *metadata_table.csv* - Metadata table of all files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 244,
     "status": "ok",
     "timestamp": 1738683866255,
     "user": {
      "displayName": "guy weintraub",
      "userId": "05496095864938298042"
     },
     "user_tz": -120
    },
    "id": "1J8hQiGFjfi-",
    "outputId": "f7609795-68d9-472f-c51c-f06ee51ef6c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Archive:  /content/hardware_materials_with_summaries.zip\n",
      "   creating: __MACOSX/\n",
      "   creating: __MACOSX/materials/\n",
      "  inflating: __MACOSX/materials/._.DS_Store  \n",
      "   creating: __MACOSX/materials/subtitles/\n",
      "  inflating: __MACOSX/materials/subtitles/._.DS_Store  \n",
      "   creating: __MACOSX/materials/texts/\n",
      "  inflating: __MACOSX/materials/texts/._.DS_Store  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_10_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_11a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_11b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_12a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_12b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_1a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_1b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_2a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_2b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_3a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_3b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_4a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_4b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_5a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_5b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_6a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_6b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_7a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_7b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_8a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_8b_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_9a_en.txt  \n",
      "  inflating: __MACOSX/materials/texts/._lecture_9b_en.txt  \n",
      "   creating: materials/\n",
      "  inflating: materials/.DS_Store     \n",
      "   creating: materials/recitations/\n",
      "  inflating: materials/recitations/Recitation_1.txt  \n",
      "  inflating: materials/recitations/Recitation_2.txt  \n",
      "  inflating: materials/recitations/Recitation_3.txt  \n",
      "  inflating: materials/recitations/Recitation_4.txt  \n",
      "  inflating: materials/recitations/Recitation_5.txt  \n",
      "  inflating: materials/recitations/Recitation_6.txt  \n",
      "  inflating: materials/recitations/Recitation_7.txt  \n",
      "  inflating: materials/recitations/Recitation_8.txt  \n",
      "   creating: materials/subtitles/\n",
      "  inflating: materials/subtitles/.DS_Store  \n",
      "  inflating: materials/subtitles/lecture_10_en.srt  \n",
      "  inflating: materials/subtitles/lecture_11a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_11b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_12a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_12b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_1a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_1b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_2a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_2b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_3a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_3b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_4a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_4b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_5a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_5b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_6a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_6b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_7a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_7b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_8a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_8b_en.srt  \n",
      "  inflating: materials/subtitles/lecture_9a_en.srt  \n",
      "  inflating: materials/subtitles/lecture_9b_en.srt  \n",
      "  inflating: materials/subtitles/srt_to_link.json  \n",
      "   creating: materials/summaries/\n",
      "  inflating: materials/summaries/summary_of_lecture_10_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_11_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_12_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_1_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_2_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_3_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_4_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_5_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_6_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_7_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_8_en.txt  \n",
      "  inflating: materials/summaries/summary_of_lecture_9_en.txt  \n",
      "   creating: materials/texts/\n",
      "  inflating: materials/texts/.DS_Store  \n",
      "  inflating: materials/texts/lecture_10_en.txt  \n",
      "  inflating: materials/texts/lecture_11a_en.txt  \n",
      "  inflating: materials/texts/lecture_11b_en.txt  \n",
      "  inflating: materials/texts/lecture_12a_en.txt  \n",
      "  inflating: materials/texts/lecture_12b_en.txt  \n",
      "  inflating: materials/texts/lecture_1a_en.txt  \n",
      "  inflating: materials/texts/lecture_1b_en.txt  \n",
      "  inflating: materials/texts/lecture_2a_en.txt  \n",
      "  inflating: materials/texts/lecture_2b_en.txt  \n",
      "  inflating: materials/texts/lecture_3a_en.txt  \n",
      "  inflating: materials/texts/lecture_3b_en.txt  \n",
      "  inflating: materials/texts/lecture_4a_en.txt  \n",
      "  inflating: materials/texts/lecture_4b_en.txt  \n",
      "  inflating: materials/texts/lecture_5a_en.txt  \n",
      "  inflating: materials/texts/lecture_5b_en.txt  \n",
      "  inflating: materials/texts/lecture_6a_en.txt  \n",
      "  inflating: materials/texts/lecture_6b_en.txt  \n",
      "  inflating: materials/texts/lecture_7a_en.txt  \n",
      "  inflating: materials/texts/lecture_7b_en.txt  \n",
      "  inflating: materials/texts/lecture_8a_en.txt  \n",
      "  inflating: materials/texts/lecture_8b_en.txt  \n",
      "  inflating: materials/texts/lecture_9a_en.txt  \n",
      "  inflating: materials/texts/lecture_9b_en.txt  \n"
     ]
    }
   ],
   "source": [
    "!unzip /content/hardware_materials_with_summaries.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WWvGvf_WQJto"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# path to the CSV file containing the metadata\n",
    "file_path = '/content/metadata_table.csv'\n",
    "\n",
    "# Load the CSV into a pandas DataFrame\n",
    "df_metadata = pd.read_csv(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 224,
     "status": "ok",
     "timestamp": 1738683872224,
     "user": {
      "displayName": "guy weintraub",
      "userId": "05496095864938298042"
     },
     "user_tz": -120
    },
    "id": "aJUYwSuqiqpi",
    "outputId": "acb60836-6201-45ad-c796-81b88849397e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OK - Loaded files are the same as the csv metadata table (duplicates ignored).\n"
     ]
    }
   ],
   "source": [
    "# Load all the *.txt* files (with the metadata) as a list of Documents\n",
    "PATH_TO_TEXTS = '/content/materials/texts'\n",
    "PATH_TO_RECITATIONS = '/content/materials/recitations'\n",
    "PATH_TO_SUMMARIES  = '/content/materials/summaries'\n",
    "\n",
    "\n",
    "text_files = [f for f in os.listdir(PATH_TO_TEXTS) if f.endswith('.txt')]\n",
    "recitation_files = [f for f in os.listdir(PATH_TO_RECITATIONS) if f.endswith('.txt')]\n",
    "summary_files = [f for f in os.listdir(PATH_TO_SUMMARIES) if f.endswith('.txt')]\n",
    "\n",
    "all_files_read = text_files + recitation_files + summary_files\n",
    "\n",
    "## verify that all read files are in the meta_data table\n",
    "if set(all_files_read) == set(df_metadata['file_name']):\n",
    "    print(\"OK - Loaded files are the same as the csv metadata table (duplicates ignored).\")\n",
    "else:\n",
    "    print(\"Warning - Loaded files are not the same as the csv metadata table\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9kds2VxXpCJV"
   },
   "outputs": [],
   "source": [
    "docs = []\n",
    "loaders = []\n",
    "\n",
    "## read files and meta data\n",
    "def read_files_and_metadata(path: str, files_list: list):\n",
    "  for f in files_list:\n",
    "      loader = TextLoader(f\"{path}/{f}\")\n",
    "      file_docs = loader.load()\n",
    "      for doc in file_docs:\n",
    "        metadata = extract_metadata_from_df(f, df_metadata) # helper function\n",
    "        doc.metadata.update(metadata)\n",
    "      docs.extend(file_docs)\n",
    "\n",
    "read_files_and_metadata(PATH_TO_TEXTS, text_files)\n",
    "read_files_and_metadata(PATH_TO_RECITATIONS, recitation_files)\n",
    "read_files_and_metadata(PATH_TO_SUMMARIES, summary_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu0_jvqmnkfv"
   },
   "source": [
    "## Add documents to the PINECONE vector_store"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "X4GJX0VfniAI"
   },
   "outputs": [],
   "source": [
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n",
    "all_splits = text_splitter.split_documents(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lQClrp_-vYEx"
   },
   "outputs": [],
   "source": [
    "# Index chunks into vector DB\n",
    "vector_store = PineconeVectorStore.from_documents(\n",
    "    all_splits,\n",
    "    embeddings,\n",
    "    index_name=\"guy-index-3-large\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 463,
     "status": "ok",
     "timestamp": 1738683911742,
     "user": {
      "displayName": "guy weintraub",
      "userId": "05496095864938298042"
     },
     "user_tz": -120
    },
    "id": "d3f8eyrRopDX",
    "outputId": "73603121-36ff-47bb-af2a-d160cbca27e7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dimension': 3072,\n",
      " 'index_fullness': 0.0,\n",
      " 'namespaces': {'': {'vector_count': 951}},\n",
      " 'total_vector_count': 951}\n"
     ]
    }
   ],
   "source": [
    "### check the vector_store:\n",
    "stats = index_pc.describe_index_stats()\n",
    "print(stats) # total with recitation and summaries 951"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bXOPEKl4Qbax"
   },
   "source": [
    "# Build Query Translation and Retrieval\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3742,
     "status": "ok",
     "timestamp": 1738683918191,
     "user": {
      "displayName": "guy weintraub",
      "userId": "05496095864938298042"
     },
     "user_tz": -120
    },
    "id": "sXRVTItFgoxT",
    "outputId": "916e0067-b3a0-4b89-9c0e-aa4988da7018"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting lark\n",
      "  Downloading lark-1.2.2-py3-none-any.whl.metadata (1.8 kB)\n",
      "Downloading lark-1.2.2-py3-none-any.whl (111 kB)\n",
      "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/111.0 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[91m━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[91m╸\u001b[0m\u001b[90m━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.7/111.0 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m111.0/111.0 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: lark\n",
      "Successfully installed lark-1.2.2\n"
     ]
    }
   ],
   "source": [
    "!pip install lark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PVRvgbkurl5P"
   },
   "outputs": [],
   "source": [
    "from langchain_core.structured_query import StructuredQuery, Operation, Comparator, Comparison\n",
    "from langchain.chains.query_constructor.base import AttributeInfo, get_query_constructor_prompt, StructuredQueryOutputParser\n",
    "from langchain.retrievers.self_query.pinecone import PineconeTranslator\n",
    "from lark import Lark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xY6yWdC785HD"
   },
   "source": [
    "## Define Query Constructor and Translator\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qSw8O7TAEKwf"
   },
   "source": [
    "**Define parameters to build specialized query constructor:**\n",
    "\n",
    "1) document_content_description\n",
    "\n",
    "2) allowed_comparators\n",
    "\n",
    "3) examples of parsing user queries (few shot learning)\n",
    "\n",
    "4) metadata fields definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gItXkS5at6bO"
   },
   "outputs": [],
   "source": [
    "curr_date = date.today()\n",
    "\n",
    "document_content_description = \"University course materials. The materials are transcripts and notes that contain all the material taught by the staff.\"\n",
    "\n",
    "# Define allowed comparators list\n",
    "allowed_comparators = [\n",
    "    \"$eq\",  # Equal to (number, string, boolean)\n",
    "    \"$ne\",  # Not equal to (number, string, boolean)\n",
    "    \"$gt\",  # Greater than (number)\n",
    "    \"$gte\",  # Greater than or equal to (number)\n",
    "    \"$lt\",  # Less than (number)\n",
    "    \"$lte\",  # Less than or equal to (number)\n",
    "    \"$in\",  # In array (string or number)\n",
    "    \"$nin\",  # Not in array (string or number)\n",
    "    \"$exists\", # Has the specified metadata field (boolean)\n",
    "    \"$and\", # Combines multiple filters\n",
    "]\n",
    "\n",
    "# Desired MetaData fields to detect\n",
    "metadata_field_info = [\n",
    "    AttributeInfo(\n",
    "        name=\"source_type\",\n",
    "        description=\"Type of source: lecture (lesson given by the professor) / recitation (by the Teaching Assistant)\", # / lecture_notes / presentation\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"number\",\n",
    "        description=\"Number of lesson source (1 being first). Meaning the order of the classes as they were taught\",\n",
    "        type=\"int\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"date\",\n",
    "        description=\"Date of original class, saved as string YYYY-MM-DD\",\n",
    "        type=\"string\",\n",
    "    ),\n",
    "    AttributeInfo(\n",
    "        name=\"is_summary\",\n",
    "        description=\"Summary of the lesson, containing the topics talked about. Whenever a user asks about topics, what we talked or learned about, what was in or about, summary of a lecture or a lesson, is_summary should be True in the filter to ensure summaries are retrieved.\",\n",
    "        type=\"bool\",\n",
    "    )\n",
    "]\n",
    "\n",
    "# Examples for few-shot learning\n",
    "examples = [\n",
    "    (\n",
    "        \"Can you give me some examples of using flip-flops in adders that we talked about in lecture 3?\",\n",
    "        {\n",
    "            \"query\": \"examples of using flip-flops in adders\",\n",
    "            \"filter\": 'and(in(\"source_type\", [\"lecture\"]),in(\"number\", [3]))',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What kinds of architecture did we learn about in the recitations?\",\n",
    "        {\n",
    "            \"query\": \"kinds of architecture\",\n",
    "            \"filter\": 'in(\"source_type\", [\"recitation\"])',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Can you give me some examples of using flip-flops in adders that we talked about in lecture 3 or 5?\",\n",
    "        {\n",
    "            \"query\": \"examples of using flip-flops in adders\",\n",
    "            \"filter\": 'and(in(\"source_type\", [\"lecture\"]),in(\"number\", [3, 5]))',\n",
    "        },\n",
    "    ),\n",
    "        (\n",
    "        \"What did we learn about Half-Adders until week 5?\",\n",
    "        {\n",
    "            \"query\": \"Half-Adders\",\n",
    "            \"filter\": 'lte(\"number\", 5)',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What did we learn about Half-Adders before september 20, 2023?\",\n",
    "        {\n",
    "            \"query\": \"Half-Adders\",\n",
    "            \"filter\": 'lt(\"date\", \"2023-09-20\")',\n",
    "        },\n",
    "    ),\n",
    "        (\n",
    "        \"What did we learn about Half-Adders after september 20, 2023?\",\n",
    "        {\n",
    "            \"query\": \"Half-Adders\",\n",
    "            \"filter\": 'gt(\"date\", \"2023-09-20\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What did we learn about Half-Adders in 20.10?\",\n",
    "        {\n",
    "            \"query\": \"Half-Adders\",\n",
    "            \"filter\": 'eq(\"date\", \"2024-10-20\")',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"Summarize lecture 8\",\n",
    "        {\n",
    "            \"query\": \"\",\n",
    "            \"filter\": 'and(in(\"source_type\", [\"lecture\"]), eq(\"is_summary\", True) ,in(\"number\", [8]))',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What did we talk about in lecture 8\",\n",
    "        {\n",
    "            \"query\": \"\",\n",
    "            \"filter\": 'and(in(\"source_type\", [\"lecture\"]), eq(\"is_summary\", True) ,in(\"number\", [8]))',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What are the main topics discussed\\talked in lecture eight\",\n",
    "        {\n",
    "            \"query\": \"\",\n",
    "            \"filter\": 'and(in(\"source_type\", [\"lecture\"]), eq(\"is_summary\", True) ,in(\"number\", [8]))',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What did we prove in lecture eight\",\n",
    "        {\n",
    "            \"query\": \"proofs\",\n",
    "            \"filter\": 'and(in(\"source_type\", [\"lecture\"]), eq(\"is_summary\", True) ,in(\"number\", [8]))',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What did we learned in the lecture yesterday\",\n",
    "        {\n",
    "            \"query\": \"\",\n",
    "            \"filter\": f'and(in(\"source_type\", [\"lecture\"]), eq(\"is_summary\", True) ,eq(\"date\", \"{curr_date-timedelta(days=1)}\"))',\n",
    "        },\n",
    "    ),\n",
    "    (\n",
    "        \"What did we learned in the lecture last week\",\n",
    "        {\n",
    "            \"query\": \"\",\n",
    "            \"filter\": f'and(in(\"source_type\", [\"lecture\"]), eq(\"is_summary\", True) ,gte(\"date\", \"{get_last_week_range()[0]}\"), lte(\"date\", \"{get_last_week_range()[1]}\"))',\n",
    "        },\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "id": "v2uwe7ltFGJO"
   },
   "outputs": [],
   "source": [
    "# Create constructor prompt using all the defined parameters\n",
    "constructor_prompt = get_query_constructor_prompt(\n",
    "    document_content_description,\n",
    "    metadata_field_info,\n",
    "    allowed_comparators=allowed_comparators,\n",
    "    examples=examples,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RzI77Xb9FR8e"
   },
   "outputs": [],
   "source": [
    "# Create query constructor\n",
    "output_parser = StructuredQueryOutputParser.from_components()\n",
    "query_constructor = constructor_prompt | llm | output_parser # llm previously defined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7bIPVA4lSLcs"
   },
   "outputs": [],
   "source": [
    "# Create an instance of PineconeTranslator\n",
    "pc_translator = PineconeTranslator()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-ZK-Kz9cgZg4"
   },
   "source": [
    "## Translation and Retrieval Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RiP9o7BDs6C6"
   },
   "outputs": [],
   "source": [
    "def translate_user_query(user_query, query_constructor, pc_translator, print_flag = False):\n",
    "  '''\n",
    "  Uses the query_constructor and pc_translator to \"translate\" the user query.\n",
    "  (Pinecone-compatible format).\n",
    "\n",
    "  * Changes the date from (str) to epoch time (int)\n",
    "  '''\n",
    "  # Step 1: Parse the query using the query constructor\n",
    "  parsed_query = query_constructor.invoke(user_query)\n",
    "\n",
    "  # Step 2: Translate the parsed query into a Pinecone-compatible format\n",
    "  translated_query = pc_translator.visit_structured_query(parsed_query)\n",
    "\n",
    "  # Step 3: Check if the user's translated query contains a date, if so change it to epoch time\n",
    "  update_date_in_filter_to_epoch(translated_query[1])\n",
    "\n",
    "  if print_flag:\n",
    "    print_translated_query(translated_query)\n",
    "\n",
    "  return translated_query\n",
    "\n",
    "\n",
    "def retrieve_docs_by_translated_query(translated_query, k = 5, print_flag = False):\n",
    "  '''\n",
    "  Retrieves results (Documents) by the translated_query.\n",
    "  Creates the filter_criteria if there are any filters, o.w None.\n",
    "  default is k=5\n",
    "  '''\n",
    "  filter_criteria = translated_query[1].get(\"filter\", None) if translated_query[1] else None\n",
    "  retrieved_docs = vector_store.similarity_search(\n",
    "      query=translated_query[0],\n",
    "      filter=filter_criteria,\n",
    "      k=k  # Number of documents to retrieve\n",
    "  )\n",
    "\n",
    "  if print_flag:\n",
    "    print_retrieved_documents(retrieved_docs)\n",
    "\n",
    "  return retrieved_docs\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TYs7dCTXq7MZ"
   },
   "source": [
    "# Build RAG Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0zyD-wIOrPE8"
   },
   "source": [
    "## Inner RAG prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 226,
     "status": "ok",
     "timestamp": 1738683936405,
     "user": {
      "displayName": "guy weintraub",
      "userId": "05496095864938298042"
     },
     "user_tz": -120
    },
    "id": "ZnZDbfmInxXy",
    "outputId": "36fc1ca1-27b3-4e84-95c3-4df021f09586"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, or the answer does not appear in the context, just say that you don't know or the answer is not in the context. Keep the answer concise.\n",
      "Question: {question} \n",
      "Context: {context} \n",
      "Answer:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/langsmith/client.py:253: LangSmithMissingAPIKeyWarning: API key must be provided when using hosted LangSmith API\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "prompt = hub.pull(\"rlm/rag-prompt\")\n",
    "prompt.messages[0].prompt.template = \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, or the answer does not appear in the context, just say that you don't know or the answer is not in the context. Keep the answer concise.\\nQuestion: {question} \\nContext: {context} \\nAnswer:\"\n",
    "print(prompt.messages[0].prompt.template)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aIbIwFcViT4_"
   },
   "source": [
    "## Graph Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rqHNfFuvdpNA"
   },
   "outputs": [],
   "source": [
    "# Define state for application\n",
    "class State(TypedDict):\n",
    "    valid_flag: bool\n",
    "    question: str\n",
    "    adapted_question: str\n",
    "    context: List[Document]\n",
    "    answer: str\n",
    "\n",
    "\n",
    "# Define application steps\n",
    "def retrieve(state: State):\n",
    "  '''\n",
    "  Retrieves documents based on the translated query.\n",
    "  '''\n",
    "  valid_flag = True\n",
    "  answer_temp = \"\"\n",
    "  adapted_query = state[\"question\"]\n",
    "\n",
    "  # 1: translate query\n",
    "  translated_query = translate_user_query(\n",
    "      state[\"question\"],\n",
    "      query_constructor,\n",
    "      pc_translator)\n",
    "\n",
    "  ### 2: Query Analyzer - Use translated query to update\n",
    "  # update query\n",
    "  adapted_query = get_adapted_query_by_translated_query(translated_query, adapted_query)\n",
    "\n",
    "  # update filter\n",
    "  update_filter_by_translated_query(translated_query)\n",
    "\n",
    "  # choose adaptive k\n",
    "  k = choose_k_by_translated_query(translated_query)\n",
    "\n",
    "  print_translated_query(translated_query) # print?\n",
    "\n",
    "  ### Step 3: Perform hybrid retrieval using the updated params\n",
    "  retrieved_docs = retrieve_docs_by_translated_query(translated_query, k)\n",
    "\n",
    "  # if no docs met the criteria - skip generation phase\n",
    "  if len(retrieved_docs) == 0:\n",
    "    answer_temp = f'Sorry, your requested materials could not be found\\n{get_filter_string(translated_query[1])}'\n",
    "    valid_flag = False\n",
    "\n",
    "  # Update state with the retrieved documents\n",
    "  return {\n",
    "    \"valid_flag\": valid_flag,           # Update valid_flag field\n",
    "    \"adapted_question\": adapted_query,  # Update question field\n",
    "    \"context\": retrieved_docs,          # Update context field\n",
    "    \"answer\": answer_temp\n",
    "  }\n",
    "\n",
    "\n",
    "def generate(state: State):\n",
    "    if not state[\"valid_flag\"]: # skip generation if invalid\n",
    "        return\n",
    "    ## format string of docs together with metadata\n",
    "    llm = ChatOpenAI(model=\"gpt-4o-mini\")\n",
    "    docs_content = get_context_string_from_retrieved_docs(state[\"context\"])\n",
    "    messages = prompt.invoke({\"question\": state[\"adapted_question\"], \"context\": docs_content}) # changed\n",
    "    response = llm.invoke(messages)\n",
    "    return {\"answer\": response.content}\n",
    "\n",
    "\n",
    "# Compile application and test\n",
    "graph_builder = StateGraph(State).add_sequence([retrieve, generate])\n",
    "graph_builder.add_edge(START, \"retrieve\")\n",
    "graph = graph_builder.compile()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ow372Kp6pI2M"
   },
   "source": [
    "## Response formatting utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xxmOBLU_l0xg"
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import json\n",
    "\n",
    "def get_srt_file(srt_file_path):\n",
    "    try:\n",
    "        with open(srt_file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "            srt_content = file.readlines()\n",
    "        return srt_content\n",
    "    except FileNotFoundError:\n",
    "        return []\n",
    "\n",
    "def srt_to_url(srt_file):\n",
    "    # search for srt_to_link.json file in the same directory as the srt_file\n",
    "    srt_to_link_file = os.path.join(os.path.dirname(srt_file), \"srt_to_link.json\")\n",
    "    try:\n",
    "        with open(srt_to_link_file, \"r\", encoding=\"utf-8\") as file:\n",
    "            srt_to_link = json.load(file)\n",
    "        return srt_to_link\n",
    "    except FileNotFoundError:\n",
    "        return {}\n",
    "\n",
    "def extract_first_sentence(input_string):\n",
    "    result = re.sub(r\"^\\d*\\n*\", \"\", input_string)\n",
    "    return result.split(\"\\n\")[0]\n",
    "\n",
    "def clean_sentence(sentence):\n",
    "    # Remove punctuation and convert to lowercase\n",
    "    return re.sub(r'[^\\w\\s]', '', sentence.lower()).strip()\n",
    "\n",
    "def get_timestamp_from_srt(sentence, srt, threshold=0.9):\n",
    "    cleaned_sentence = clean_sentence(sentence)\n",
    "\n",
    "    for i, line in enumerate(srt):\n",
    "        cleaned_line = clean_sentence(line)\n",
    "\n",
    "        if len(cleaned_line) > 0 and (\n",
    "                cleaned_sentence in cleaned_line or token_similarity(cleaned_sentence, cleaned_line) >= threshold):\n",
    "            return srt[i - 1].strip().split(\",\")[0]\n",
    "    return \"NaN\"\n",
    "\n",
    "\n",
    "\n",
    "def remove_leading_zeros_timestamp(timestamp):\n",
    "    if timestamp.startswith(\"00:\"):\n",
    "        return timestamp[3:]  # Remove the first 3 characters, which are \"00:\"\n",
    "    return timestamp\n",
    "\n",
    "\n",
    "def convert_timestamp_to_elapsed_seconds(timestamp: str) -> int:\n",
    "    parts = list(map(int, timestamp.split(\":\")))\n",
    "    if len(parts) == 2:  # M:S format\n",
    "        return parts[0] * 60 + parts[1]\n",
    "    elif len(parts) == 3:  # HH:MM:SS format\n",
    "        return parts[0] * 3600 + parts[1] * 60 + parts[2]\n",
    "    else:\n",
    "        raise ValueError(\"Invalid timestamp format\")\n",
    "\n",
    "def token_similarity(sentence1, sentence2):\n",
    "    tokens1 = set(sentence1.split())\n",
    "    tokens2 = set(sentence2.split())\n",
    "    return len(tokens1.intersection(tokens2)) / len(tokens1.union(tokens2))\n",
    "\n",
    "def generate_link(url, timestamp):\n",
    "    if timestamp is None:\n",
    "        timestamp = 0\n",
    "    link = f\"{url}&t={convert_timestamp_to_elapsed_seconds(timestamp)}\"\n",
    "    return link\n",
    "\n",
    "def generate_link_text(lecture_name, first_timestamp):\n",
    "    # link (Lecture 6b 20:23-20:50)\n",
    "    first_timestamp = remove_leading_zeros_timestamp(first_timestamp)\n",
    "    return f\"{lecture_name} {first_timestamp}\"\n",
    "\n",
    "def decompose_ref(ref):\n",
    "  source = ref.metadata.get('source')\n",
    "  if 'summary' or 'Recitation' in source:\n",
    "    return f\"{source.split('/')[-1]}\"\n",
    "  elif source.lower().endswith('.pdf'):\n",
    "    return f\"{source.split('/')[-1]} - Page : {ref.metadata['page']}\"\n",
    "  else:\n",
    "    srt_file_name = source.replace('txt', 'srt').replace('texts', 'subtitles')\n",
    "    srt = get_srt_file(srt_file_name)\n",
    "    first_sentence = extract_first_sentence(ref.page_content)\n",
    "    timestamp = get_timestamp_from_srt(first_sentence, srt).split(',')[0]\n",
    "    srt_to_url_dict = srt_to_url(srt_file_name)\n",
    "    srt_without_path = os.path.basename(srt_file_name)\n",
    "    lecture_data = srt_to_url_dict[srt_without_path]\n",
    "    link = generate_link(lecture_data['url'], timestamp)\n",
    "    link_text = generate_link_text(lecture_data['name'], timestamp)\n",
    "    return f\"{link_text} Link: {link}\"\n",
    "\n",
    "\n",
    "def response_generator(valid_flag, query, adapted_query, source_documents, answer, to_print=True):\n",
    "  references = '\\n'.join([decompose_ref(ref) for ref in source_documents])\n",
    "  if to_print:\n",
    "    print(f\"\"\"\n",
    "  User Query:\\n'{query}' \\n\\n\n",
    "  Adapted Query:\\n'{adapted_query}' \\n\\n\n",
    "  Answer:\\n'{answer}' \\n\\n\n",
    "  References:\\n{references}\"\"\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jzKAdKTcqb0E"
   },
   "source": [
    "# Querying the System"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_T7YfcQnr93"
   },
   "source": [
    "## benchmark queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "v4vJfwUi7q9I"
   },
   "outputs": [],
   "source": [
    "## Online Tutor benchmark questions\n",
    "original_benchmark_queries = {\n",
    "    1: \"Can a Full Adder (FA) be used as a Half Adder (HA), and why?\",\n",
    "    2: \"Is it always possible to replace OR with XOR as we did in the development of the FA?\",\n",
    "    3: \"Why is there a need to replace OR with XOR?\",\n",
    "    4: \"Is Look Ahead Carry also dependent on the delay of each component in sequence?\",\n",
    "    5: \"Why do we need Look Ahead Carry?\",\n",
    "    6: \"What is the meaning of I0,...,I4 in a multiplexer?\",\n",
    "    7: \"Why do we need a MUX at all? What are its uses?\",\n",
    "    8: \"When you implemented a logical circuit using a MUX, how did you decide on the free variable?\",\n",
    "    9: \"What happens if we input 1-1 into an SR-Latch?\",\n",
    "    10: \"With a type D-Latch, it seems like the component does nothing. Why?\",\n",
    "    11: \"What is the advantage of a JK-Latch over a T-Latch?\",\n",
    "    12: \"Why did you change the inverters to another type of gates (NORs or NANDs)?\",\n",
    "    13: \"Is there a difference between SR-Latch based on NORs vs NANDs?\",\n",
    "    14: \"Which kind of SR-Latch will we use in our class?\",\n",
    "    15: \"How does the gated SR-Latch work? / What is a gated SR-Latch?\",\n",
    "    16: \"What is the main problem with the SR-Latch design?\",\n",
    "    17: \"What is a D-Latch? / How is the D-Latch built?\",\n",
    "    18: \"How do we find the Excitation Table?\",\n",
    "    19: \"When to use the Characteristic Table and when to use the Excitation Table?\",\n",
    "    20: \"What is a JK-Latch?\",\n",
    "    21: \"What are the differences between (R-1)'s complement and R's complement?\",\n",
    "    22: \"Can you explain to me deeply about gray code and what is the purpose of it?\",\n",
    "    23: \"Can you explain why NOR is functionally complete?\",\n",
    "    24: \"What should I do when I have a sum and I have a 'carry' in the leftmost digits, that is, I have no one to transfer the 'carry' to? For example: 1+1 in the leftmost digits sum.\",\n",
    "    25: \"Hi, is NAND a complete operating system?\",\n",
    "    26: \"How to convert fractions to a decimal base?\",\n",
    "    27: \"How to convert hexadecimal base to decimal?\",\n",
    "    28: \"What is Boolean algebra?\",\n",
    "    29: \"What is the meaning of 'the aritmetica' of SM doesn't work?\",\n",
    "    30: \"What is minterm?\",\n",
    "    31: \"Can you show me the proof that f(x,y,z)=x'y'z' + xz + yz is functionally complete?\",\n",
    "    32: \"How to subtract in base 8?\",\n",
    "    33: \"How to perform subtraction in the 1's complement method?\",\n",
    "    34: \"I need to add and subtract pairs of numbers in base 8, without converting to decimal. How can I do it?\"\n",
    "}\n",
    "\n",
    "## our MetaData-related benchmark questions\n",
    "project_queries_dict = {\n",
    "    1: \"What did we prove in lecture 5?\",\n",
    "    2: \"Please summarize the 5th class\",\n",
    "    3: \"Make a detailed summary of class 5\",\n",
    "    4: \"Make a detailed summary of classes 6 to 8\",\n",
    "    5: \"What is the topic of lecture 8?\",\n",
    "    6: \"What is the topic of class 9?\",\n",
    "    7: \"What is the topic of lesson 9?\",\n",
    "    8: \"What is the topic of lecture 9?\",\n",
    "    9: \"What was class 10 about?\",\n",
    "    10: \"What was lesson 10 about?\",\n",
    "    11: \"What was lecture 10 about?\",\n",
    "    12: \"What did we learn in recitation 4?\",\n",
    "    13: \"In what class we learned about finish state machine?\",\n",
    "    14: \"In what class did we talk about T flip-flop?\",\n",
    "    15: \"What did we learn about ROM memory in lecture 7?\",\n",
    "    16: \"What did we learn about ROM memory in lecture 11?\",\n",
    "    17: \"What did we learn about Object-oriented programming in lecture 11?\",\n",
    "    18: \"Summarize the last 15 minutes of lecture 1\",\n",
    "    19: \"In lecture 6, part a, at time 13:33, the teacher said you can exchange the Or gate in a Xor gate. Explain why.\",\n",
    "    20: \"Explain what the teacher said in lecture 6, part a, at time 13:33.\",\n",
    "    21: \"What we learned in lecture in May 2, 2020?\",\n",
    "    22: \"What we learned in lecture in February 19, 2024?\",\n",
    "    23: \"What we learned in lecture in 18.3?\",\n",
    "    24: \"What did we learn in the lecture last week?\",\n",
    "    25: \"What did we learn in the recitation last week?\",\n",
    "    26: \"In lecture 11 we learned about MOS transistor, how does it combine with what we learned throughout the course?\",\n",
    "    27: \"In lesson 2 we saw the problem of two different representations of zero. In lesson 6, we talked about implementing adders, and did not address the dual representation problem. Explain.\",\n",
    "    28: \"Explain what we did learn about timing, include only things we learned up until lecture 8 (included).\",\n",
    "    29: \"Explain what we did learn about flip-flops, include only things we learned up until lecture 7 (included).\",\n",
    "    30: \"Explain what we did learn about flip-flops, include only things we learned up until lecture 8 (included).\",\n",
    "    31: \"In lecture 4 and in recitation 4 we learned about undefined states in the Karnaugh Map does not matter, explain.\"\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ip2MoSMJrDMa"
   },
   "source": [
    "## Ask a question"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Go-PBvX_qbdN",
    "outputId": "6ff3d795-f6e5-4ffe-f4d5-8636636404f5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "printing translated query:\n",
      "Content Query: flip-flops\n",
      "Filter: {'filter': {'number': {'$lte': 7}}}\n"
     ]
    }
   ],
   "source": [
    "# define which query to answer\n",
    "q_num = 29\n",
    "query = project_queries_dict[q_num]\n",
    "# query = original_benchmark_queries[q_num]\n",
    "\n",
    "print(\"printing translated query:\")\n",
    "response_new = graph.invoke({\"question\": query})\n",
    "\n",
    "print(f\"\\nprinting query and answer:\")\n",
    "response_generator(*response_new.values())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "g60qCy8id6Xl"
   },
   "source": [
    "# Save Results to file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2ZsRWkF_Or2g"
   },
   "source": [
    "Running all the queries and insert the results into csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "collapsed": true,
    "executionInfo": {
     "elapsed": 8603,
     "status": "ok",
     "timestamp": 1737289591905,
     "user": {
      "displayName": "guy weintraub",
      "userId": "05496095864938298042"
     },
     "user_tz": -120
    },
    "id": "EKjTAdjxvro1",
    "outputId": "d5eb233f-63a1-49bf-e9fe-389f94d5b39e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating answer for query 15: What did we learn about ROM memory in lecture 7?\n",
      "-----------------------------\n",
      "Generating answer for query 17: What did we learn about Object-oriented programming in lecture 11?\n",
      "-----------------------------\n",
      "file Results/results_version_5-3_20250119_12-26_partial.csv was created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import io\n",
    "import sys\n",
    "import csv\n",
    "from datetime import datetime\n",
    "\n",
    "folder_name = \"Results\"\n",
    "version = \"final\"\n",
    "costum = \"\"\n",
    "\n",
    "os.makedirs(folder_name, exist_ok=True)\n",
    "\n",
    "# Get current time formatted as YYYYMMDD_HHMMSS\n",
    "curr_time = datetime.now().strftime(\"%Y%m%d_%H-%M\")\n",
    "file_name = f\"{folder_name}/results_version_{version}_{curr_time}_{costum}.csv\"\n",
    "\n",
    "data = []\n",
    "\n",
    "def create_retreived_docs_data_for_excel():\n",
    "  cell_data = f\"Total of {len(response_new['context'])} retrieved files:\\n\"\n",
    "  index = 1\n",
    "  for document in response_new[\"context\"]:\n",
    "    cell_data += f\"{index}. file = {document.metadata['source']}, chunk_id = {document.id}.\\n\"\n",
    "    index += 1\n",
    "  return cell_data\n",
    "\n",
    "def run_invoke_and_save_output_print(query):\n",
    "  output_capture = io.StringIO()\n",
    "  try:\n",
    "    # Save the current stdout so we can restore it later\n",
    "    old_stdout = sys.stdout\n",
    "    # Redirect stdout to the StringIO object\n",
    "    sys.stdout = output_capture\n",
    "    # Call the function (it will print to the StringIO object)\n",
    "    response_new = graph.invoke({\"question\": query})\n",
    "    # Restore the original stdout\n",
    "    sys.stdout = old_stdout\n",
    "    # Get the captured output as a string\n",
    "    captured_output = output_capture.getvalue()\n",
    "  finally:\n",
    "    sys.stdout = old_stdout\n",
    "    output_capture.close()\n",
    "\n",
    "  return response_new, captured_output\n",
    "\n",
    "def get_translation(captured_output: str):\n",
    "  lines = captured_output.splitlines()\n",
    "  tranlated_lines = [line for line in lines if line.startswith(\"Translated\")]\n",
    "  translation = \"\"\n",
    "  for line in lines:\n",
    "    translation += line + \"\\n\"\n",
    "  return translation\n",
    "\n",
    "#### define which answers to test and save\n",
    "q_to_test = [15, 17]\n",
    "#### if you want to test all questions, uncomment the next line:\n",
    "# q_to_test = range(1,len(project_queries_dict)+1)\n",
    "for q_num in q_to_test:\n",
    "  query = project_queries_dict[q_num]\n",
    "  print(f\"Generating answer for query {q_num}: {query}\")\n",
    "  response_new, captured_output = run_invoke_and_save_output_print(query)\n",
    "  response_generator(*response_new.values(), to_print=False)\n",
    "  data.append({\"Query Number\": q_num, \"Original Query\": query, \"Translation\": get_translation(captured_output), \"Retrieved Docs\": create_retreived_docs_data_for_excel(), \"Adapted Query\": response_new['adapted_question'], \"Answer\": response_new['answer']})\n",
    "  print(\"-----------------------------\")\n",
    "\n",
    "with open(file_name, mode='w', newline='', encoding='utf-8') as file:\n",
    "  writer = csv.DictWriter(file, fieldnames=[\"Query Number\", \"Original Query\", \"Translation\", \"Retrieved Docs\", \"Adapted Query\", \"Answer\"])\n",
    "  writer.writeheader()\n",
    "  writer.writerows(data)\n",
    "\n",
    "print(f\"file {file_name} was created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "oL1G9cBPOttB"
   },
   "source": [
    "For convenience - convert the csv to excel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2423,
     "status": "ok",
     "timestamp": 1737288968058,
     "user": {
      "displayName": "guy weintraub",
      "userId": "05496095864938298042"
     },
     "user_tz": -120
    },
    "id": "y7vQnyjlOvNx",
    "outputId": "6013a4ac-ee8c-46e8-92af-aabaf41429e9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
      "Requirement already satisfied: openpyxl in /usr/local/lib/python3.11/dist-packages (3.1.5)\n",
      "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (1.26.4)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2024.2)\n",
      "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.11/dist-packages (from openpyxl) (2.0.0)\n",
      "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pandas openpyxl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 337,
     "status": "ok",
     "timestamp": 1737289596206,
     "user": {
      "displayName": "guy weintraub",
      "userId": "05496095864938298042"
     },
     "user_tz": -120
    },
    "id": "LrluoH2vOvYB",
    "outputId": "eb0d929a-8085-4088-d9f5-7ff299f65634"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Converted Results/results_version_5-3_20250119_12-26_partial.csv to Results/results_version_5-3_20250119_12-26_partial.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "xlsx_file = file_name.replace(\".csv\", \".xlsx\")\n",
    "\n",
    "data = pd.read_csv(file_name)\n",
    "\n",
    "data.to_excel(xlsx_file, index=False, engine='openpyxl')\n",
    "\n",
    "print(f\"Converted {file_name} to {xlsx_file}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": [
    {
     "file_id": "1pa5vf0cNZxiPr3MGvCJjBF1daNRqCXAH",
     "timestamp": 1736168423956
    },
    {
     "file_id": "1OWbcdHZpiTFHsgklwE_rzfoT9HyO3SId",
     "timestamp": 1735562120575
    },
    {
     "file_id": "1XIqv4m0MwljIL7xrNKe7Ahz7naaLpD31",
     "timestamp": 1734536166850
    },
    {
     "file_id": "1iUjduPN4owA2l2kN8Z_8siMkokegk9eo",
     "timestamp": 1733498625479
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
